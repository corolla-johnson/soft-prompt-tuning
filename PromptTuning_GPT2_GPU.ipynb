{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PromptTuning-GPT2-GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZMlpJSa5LYW"
      },
      "source": [
        "# What's Prompt Tuning?\n",
        "Prompt tuning optimizes a set of 20-100 special tokens at the start of the context in order to replicate the effect of a finetuned language model. These tokens are continuous vector embeddings that can't be decoded into words, but can still force the model to behave in a certain way.\n",
        "\n",
        "Unfortunately, the transformers training and generation utilities don't yet support embeddings as input, so I've set up rudimentary training and generation loops for a simple description task.\n",
        "\n",
        "You can read more here:\n",
        "\n",
        "https://arxiv.org/abs/2104.08691\n",
        "\n",
        "# What's this sheet do?\n",
        "This sheet finds a set of special tokens that forces the model to output a given description for a character or object.\n",
        "\n",
        "If you've played around with prompt engineering in AI Dungeon, it's like getting a computer to write a compressed, non-human-readable World Info entry based on your prose description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPIpIH6a6VVR",
        "cellView": "form"
      },
      "source": [
        "#@title Setup dependencies\n",
        "!pip install transformers\n",
        "!git clone https://github.com/corolla-johnson/soft-prompt-tuning.git soft_prompt_tuning\n",
        "!nvidia-smi\n",
        "\n",
        "model_setup_for_prompt_tuning = False\n",
        "\n",
        "# Setup word wrapping\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxqGzAMaiHnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "outputId": "938f4b86-9e6b-497f-8d49-2cb025dd6b5d"
      },
      "source": [
        "#@title Grab model\n",
        "from transformers import GPT2LMHeadModel, GPTNeoForCausalLM, GPT2TokenizerFast\n",
        "from transformers.optimization import Adafactor\n",
        "from soft_prompt_tuning.soft_embedding import SoftEmbedding\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(\"cuda\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAVYspvhLApX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "cellView": "form",
        "outputId": "2d425f26-2a2f-41eb-a06d-463b0a9dfdc3"
      },
      "source": [
        "#@title Input target description\n",
        "#@markdown Double-click this cell to open it and edit the 'prefix_len', 'prompt' and 'target' variables.\n",
        "\n",
        "# Length of the tuned prompt in tokens.\n",
        "# The paper doesn't recommend going over 100.\n",
        "prefix_len = 20\n",
        "\n",
        "# Fixed part of the prompt (how you ask the model for a description)\n",
        "prompt = \"Detailed description of Emma Violence:\\n\"\n",
        "\n",
        "# Desired description of the character or object\n",
        "target = (\"Emma Violence is a British cybernetic assassin who is known for her \"\n",
        "          \"elegant style and high-profile targets. She is described to be a perfectionist \"\n",
        "          \"in her work, often going above and beyond the call of duty. \"\n",
        "          \"Despite her cold, ruthless, and calculating nature, she has a warm and \"\n",
        "          \"motherly side that she only shows to a select few people. She has two guns \"\n",
        "          \"implanted to her forearms and can utilize them both with deadly accuracy.\") \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize strings\n",
        "prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "prompt_len = prompt_tokens.input_ids.shape[1]\n",
        "target_tokens = tokenizer(target, return_tensors=\"pt\")\n",
        "target_len = target_tokens.input_ids.shape[1]\n",
        "target_start = prefix_len + prompt_len\n",
        "\n",
        "print(f\"Prefix Length: {prefix_len}\")\n",
        "print(f\"Prompt Length: {prompt_len}\")\n",
        "print(f\"Target Length: {target_len}\")\n",
        "\n",
        "inputs = tokenizer(prompt)\n",
        "# need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
        "# even though it does not matter what you pad input_ids with, it's just to make HF happy\n",
        "inputs['input_ids'] = torch.cat([torch.full((1,prefix_len), 50256), prompt_tokens['input_ids'], target_tokens['input_ids']],1).cuda()\n",
        "inputs['attention_mask'] = torch.cat([torch.full((1,prefix_len), 1), prompt_tokens['attention_mask'], target_tokens['attention_mask']],1).cuda()\n",
        "labels = torch.cat([torch.full((1,target_start), -100), target_tokens['input_ids']], 1).cuda()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prefix Length: 20\n",
            "Prompt Length: 7\n",
            "Target Length: 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLSJ17bvQ5js",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "outputId": "012a5b69-3509-41c7-e81a-34f9f78ec4ae"
      },
      "source": [
        "#@title Configure model for prompt tuning\n",
        "#@markdown Runs only once\n",
        "if not model_setup_for_prompt_tuning:\n",
        "  model.train()\n",
        "\n",
        "  # Freeze model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  old_wte = model.get_input_embeddings()\n",
        "\n",
        "  # Add softembedding module\n",
        "  s_wte = SoftEmbedding(old_wte,\n",
        "                        n_tokens=prefix_len,\n",
        "                        initialize_from_vocab=True).to(\"cuda\")\n",
        "  model.set_input_embeddings(s_wte)\n",
        "\n",
        "  # Set up optimizer\n",
        "  params = [model.transformer.wte.learned_embedding]\n",
        "  optimizer = Adafactor(params=params)\n",
        "  model_setup_for_prompt_tuning = True\n",
        "\n",
        "  # Test model output\n",
        "  output = model(**inputs, labels=labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N89nOW4rRZWR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "cellView": "form",
        "outputId": "51c7e5e2-e447-42c1-fd4f-f735b5036e65"
      },
      "source": [
        "#@title (OPTIONAL) Load existing prefix\n",
        "#@markdown This will override the existing prefix_len with the size of the loaded one.\n",
        "\n",
        "path = \"learned_prefix.pt\"#@param{type:\"string\"}\n",
        "s_wte.learned_embedding = torch.load(path)\n",
        "\n",
        "prefix_len = s_wte.learned_embedding.shape[0]\n",
        "s_wte.n_tokens = prefix_len\n",
        "\n",
        "print(f\"Loaded prefix of length {prefix_len}\")\n",
        "output = model(**inputs, labels=labels)\n",
        "loss = output.loss\n",
        "loss.backward()\n",
        "print(f\"Loss: {loss}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded prefix of length 20\n",
            "Loss: 0.7311124205589294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5arPRBUa3rF",
        "cellView": "form"
      },
      "source": [
        "#@title Training\n",
        "#@markdown 4000+ for \"gpt2\"\n",
        "#@markdown\n",
        "#@markdown 200+ for \"GPT-Neo-2.7B\"\n",
        "\n",
        "iterations = 4000#@param{type:\"number\"}\n",
        "\n",
        "for i in range(iterations):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(**inputs, labels=labels)\n",
        "  loss = output.loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if i%10 == 0:\n",
        "    print(f\"{i}: Loss: {loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu4rD__3Rp__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "cellView": "form",
        "outputId": "fad6f814-c325-48d4-f5a1-376cf601f042"
      },
      "source": [
        "#@title (OPTIONAL) Save tuned prefix\n",
        "path = \"learned_prefix.pt\"#@param{type:\"string\"}\n",
        "torch.save(model.transformer.wte.learned_embedding, path)\n",
        "\n",
        "print(f\"Saved prefix of length {prefix_len}\")\n",
        "output = model(**inputs, labels=labels)\n",
        "loss = output.loss\n",
        "loss.backward()\n",
        "print(f\"Loss: {loss}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saved prefix of length 20\n",
            "Loss: 0.6657297611236572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "lwKSO0ZKX6-s",
        "outputId": "f0c4e05e-e69c-4ca5-fa30-002782e83f1b"
      },
      "source": [
        "#@title (OPTIONAL) Reinitialize prefix\n",
        "#@markdown Warning: This will reset any training. Make sure to save the tuned prefix first.\n",
        "s_wte.__init__(old_wte, s_wte.n_tokens)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "Nv_ZDWerGxXK",
        "outputId": "9630b3b0-dcc5-4143-9995-f90760bbe394"
      },
      "source": [
        "#@title Generation parameters \n",
        "#@markdown Make sure to run this cell after making changes to the parameters.\n",
        "\n",
        "use_prefix = True #@param{type:\"boolean\"}\n",
        "\n",
        "custom_prompt = \"Emma propped the sniper rifle on a balustrade overlooking the Rue de la Paix and carefully aligned her crosshairs with the target. He was a man in a\" #@param{type:\"string\"}\n",
        "use_custom_prompt = False #@param{type:\"boolean\"}\n",
        "\n",
        "temperature = 0.7 #@param{type:\"number\"}\n",
        "top_k = 0.7 #@param{type:\"number\"}\n",
        "\n",
        "output_length = 120 #@param{type:\"number\"}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "NiaIlO1MFwqq",
        "cellView": "form",
        "outputId": "44618839-ec06-486d-cd74-1586f56905d0"
      },
      "source": [
        "#@title Generate!\n",
        "#@markdown We're using a fairly barebones top-k sampling scheme so the output might be quite repetitive.\n",
        "model.eval()\n",
        "\n",
        "if use_prefix:\n",
        "  model.transformer.wte = s_wte\n",
        "\n",
        "  if use_custom_prompt:\n",
        "    test_inputs = tokenizer(custom_prompt, return_tensors=\"pt\")\n",
        "    test_inputs.input_ids = torch.cat([torch.full((1,prefix_len), 50256), test_inputs['input_ids']],1).cuda()\n",
        "    test_inputs.attention_mask = torch.cat([torch.full((1,prefix_len), 1), test_inputs['attention_mask']],1).cuda()\n",
        "\n",
        "  else:\n",
        "    test_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    test_inputs.input_ids = torch.cat([torch.full((1,prefix_len), 50256), prompt_tokens['input_ids']],1).cuda()\n",
        "    test_inputs.attention_mask = torch.cat([torch.full((1,prefix_len), 1), prompt_tokens['attention_mask']],1).cuda()\n",
        "\n",
        "else:\n",
        "  model.transformer.wte = old_wte\n",
        "  if use_custom_prompt:\n",
        "    test_inputs = tokenizer(custom_prompt, return_tensors=\"pt\")\n",
        "    test_inputs.input_ids = test_inputs.input_ids.cuda()\n",
        "    test_inputs.attention_mask = test_inputs.attention_mask.cuda()\n",
        "  else:\n",
        "    test_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    test_inputs.input_ids = test_inputs.input_ids.cuda()\n",
        "    test_inputs.attention_mask = test_inputs.attention_mask.cuda()\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "with torch.no_grad():\n",
        "  for k in range(output_length):\n",
        "      import random\n",
        "      import numpy as np\n",
        "      import torch\n",
        "      import torch.nn as nn\n",
        "      from torch.nn import functional as F\n",
        "\n",
        "      logits = model(input_ids=test_inputs.input_ids.cuda(), attention_mask=test_inputs.attention_mask.cuda()).logits\n",
        "      \n",
        "      # pluck the logits at the final step and scale by temperature\n",
        "      logits = logits[:, -1, :] / temperature\n",
        "\n",
        "      logits = top_k_logits(logits, 10)\n",
        "      \n",
        "      # apply softmax to convert to probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      # sample from the distribution\n",
        "      next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      # append to the sequence and continue\n",
        "      test_inputs.input_ids = torch.cat((test_inputs.input_ids, next_token), dim=1).cuda()\n",
        "      test_inputs.attention_mask = torch.cat([test_inputs.attention_mask, torch.full((1,1), 1).cuda()],1).cuda()\n",
        "\n",
        "print(f\"Target:\\n<{prefix_len} token tuned prompt>\\n{prompt + target}\\n\")\n",
        "\n",
        "if use_prefix:\n",
        "  print(f\"Actual:\\n<{prefix_len} token tuned prompt>\\n{tokenizer.decode(test_inputs.input_ids[0][prefix_len:])}\")\n",
        "else:\n",
        "  print(f\"Actual:\\n{tokenizer.decode(test_inputs.input_ids[0])}\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Target:\n",
            "<20 token tuned prompt>\n",
            "Detailed description of Emma Violence:\n",
            "Emma Violence is a British cybernetic assassin who is known for her elegant style and high-profile targets. She is described to be a perfectionist in her work, often going above and beyond the call of duty. Despite her cold, ruthless, and calculating nature, she has a warm and motherly side that she only shows to a select few people. She has two guns implanted to her forearms and can utilize them both with deadly accuracy.\n",
            "\n",
            "Actual:\n",
            "<20 token tuned prompt>\n",
            "Detailed description of Emma Violence:\n",
            "Emma Violence is a British cybernetic assassin who is known for her elegant style and high-profile targets. She is described to be a perfectionist in her work and is known for her elegant style. Despite her cold, calculated, and ruthless approach to a target, she has two guns implanted to her forearms and can utilize them both with deadly accuracy. Despite her cold, calculated, and ruthless approach to a target, she has a warm and motherly side that she only shows to a select few people. While her cold, calculated, and ruthless approach to a target is not known to them\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}